{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification Model for Tripadvisor Hotel Review Star Ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rushej2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ta_dataset = load_dataset(\"jniimi/tripadvisor-review-rating\")['train']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted data and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [item['text'] for item in ta_dataset]\n",
    "train_data_labels = [int(item['overall']) for item in ta_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data train:test 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_data_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created binary labels for One-vs-Rest classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_all_labels(labels, target_class):\n",
    "    return [1 if label == target_class else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained 5 binary classifiers using pipeline of vectorizer and classifier.\n",
    "Then, fit the classifier to the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "for star in range(1, 6):\n",
    "    binary_y_train = one_vs_all_labels(y_train, star)\n",
    "    \n",
    "\n",
    "    clf = make_pipeline(\n",
    "        TfidfVectorizer(analyzer='word',max_features=5000,lowercase=True),\n",
    "        LogisticRegression(max_iter=1000)\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, binary_y_train)\n",
    "    classifiers[star] = clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluated the classifiers against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for star rating: 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     38693\n",
      "           1       0.75      0.46      0.57      1566\n",
      "\n",
      "    accuracy                           0.97     40259\n",
      "   macro avg       0.86      0.73      0.78     40259\n",
      "weighted avg       0.97      0.97      0.97     40259\n",
      "\n",
      "\n",
      "Evaluation for star rating: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     38112\n",
      "           1       0.43      0.11      0.18      2147\n",
      "\n",
      "    accuracy                           0.94     40259\n",
      "   macro avg       0.69      0.55      0.57     40259\n",
      "weighted avg       0.92      0.94      0.93     40259\n",
      "\n",
      "\n",
      "Evaluation for star rating: 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     34579\n",
      "           1       0.59      0.27      0.37      5680\n",
      "\n",
      "    accuracy                           0.87     40259\n",
      "   macro avg       0.74      0.62      0.65     40259\n",
      "weighted avg       0.85      0.87      0.85     40259\n",
      "\n",
      "\n",
      "Evaluation for star rating: 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.80     26790\n",
      "           1       0.60      0.41      0.49     13469\n",
      "\n",
      "    accuracy                           0.71     40259\n",
      "   macro avg       0.67      0.64      0.64     40259\n",
      "weighted avg       0.69      0.71      0.69     40259\n",
      "\n",
      "\n",
      "Evaluation for star rating: 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.83     22862\n",
      "           1       0.78      0.77      0.77     17397\n",
      "\n",
      "    accuracy                           0.80     40259\n",
      "   macro avg       0.80      0.80      0.80     40259\n",
      "weighted avg       0.80      0.80      0.80     40259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for star in range(1, 6):\n",
    "    print(f\"\\nEvaluation for star rating: {star}\")\n",
    "    binary_y_test = one_vs_all_labels(y_test, star)\n",
    "    y_pred = classifiers[star].predict(X_test)\n",
    "    print(classification_report(binary_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found binary classifier (1 star, 2 star etc.) with highest probability for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_star_rating(classifiers, review_text):\n",
    "    probabilities = {star: clf.predict_proba([review_text])[0][1] for star, clf in classifiers.items()}\n",
    "    return max(probabilities, key=probabilities.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted for the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = [predict_star_rating(classifiers, review) for review in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "overall_accuracy = accuracy_score(y_test, predicted_ratings)\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
